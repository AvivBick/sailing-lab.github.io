---
---

@article{baidu2021,
  abbr={Baidu},
  title    = "From Learning, to Meta-Learning, to &quot;Lego-Learning” -- theory, system, and applications",
  year     = 2021,
  journal = "Baidu",
  website  = "https://youtu.be/_12g9kpL4K8?t=1",
  abstract = "Software systems for complex tasks - such as controlling manufacturing processes in real-time; or writing radiological case reports within a clinical workflow – are becoming increasingly sophisticated and consist of a large number of data, model, algorithm, and system elements and modules. Traditional benchmark/leaderboard-driven bespoke approaches in the Machine Learning community are not suited to meet the highly demanding industrial standards beyond algorithmic performance, such as cost-effectiveness, safety, scalability, and automatability, typically expected in production systems. In this talk, I discuss some technical issues toward addressing these challenges: 1) a theoretical framework for trustworthy and panoramic learning with all experiences; 2) optimization methods to best the effort for learning under such a principled framework; 3) compositional strategies for building production-grade ML programs from standard parts. I will present our recent work toward developing a standard model for Learning that unifies different machine learning paradigms and algorithms, then a Bayesian blackbox optimization approach to Meta Learning in the space of hyperparameters, model architectures, and system configurations, and finally principles and designs of standardized software Legos that facilitate cost-effective building, training, and tunning of practical ML pipelines and systems."
}

@article{kdddld2021,
  abbr={KDD DLD},
  title    = "It is time for deep learning to understand its expense bills",
  year     = 2021,
  journal = "KDD Deep Learning Day",
  website  = "https://youtu.be/1ziZcHRqtNU",
  abstract = {In the past several years, deep learning has dominated both academic and industrial R&D over a wide range of applications, with two remarkable trends: 1) developing and training ever larger "all-purpose" monster models over all data possibly available, with a astounding 10,000x parameter number increase in recent 3 years; 2) developing and assembling end-to-end "white-boxes" deployments with ever larger number of component sub-models that need to be highly customized and interoperative. Progresses made to the leaderboards or featured in news headlines are highlighting metrics such as saliency of content production, accuracy on labeling, or speed of convergence, but a number of key challenges impacting the cost effectiveness of such results, and eventually the sustainability of current R&D efforts in DL, are not receiving enough attention: 1) For large models, how many lines of code outside of the DL model are need to parallelize the computing over a computer cluster? (2) Which/How many hardware resources to use to train and deploy the model? (3) How to tune the model, the code, and the system to achieve optimum performance? (4) Can we automate composition, parallelization, tuning, and resource sharing between many users and jobs? In this talk, I will discuss these issues as a core focus in SysML research, and I will present some preliminary results on how to build standardizable, adaptive, and automatable system support for DL based on first principles (when available) underlying DL design and implementation.}
}

